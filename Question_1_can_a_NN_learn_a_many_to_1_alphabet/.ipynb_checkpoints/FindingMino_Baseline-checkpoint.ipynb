{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.datasets import mnist\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models from the MNIST example in Deep Learning with Python (Chollet, 2018)\n",
    "\n",
    "- **DNN**\n",
    "\n",
    "- **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into training and test\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN():\n",
    "\n",
    "    network = tf.keras.Sequential()\n",
    "    network.add(layers.Dense(512, activation = 'relu', input_shape=(28*28,)))\n",
    "    network.add(layers.Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    network.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data into shape that network expects and transform it from range 0-255 to range 0-1\n",
    "train_images = train_images.reshape((60000, 28 * 28))/255\n",
    "test_images = test_images.reshape((10000, 28 * 28))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2590 - accuracy: 0.9251\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1062 - accuracy: 0.9688\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.9789\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0496 - accuracy: 0.9853\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15c4f9100>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs = 5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 671us/step - loss: 0.0679 - accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sampling a subset of the data, to make the combinations more tractable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 28, 56)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data so they can be fed into the network, one-hot-encode the labels\n",
    "\n",
    "train_images = combo_train_X.reshape((combo_train_X.shape[0], 28, 56, 1))\n",
    "train_images = train_images/ 255\n",
    "\n",
    "val_images = combo_val_X.reshape((combo_val_X.shape[0], 28, 56, 1))\n",
    "val_images = val_images/255\n",
    "\n",
    "train_labels = to_categorical(combo_train_y)\n",
    "val_labels = to_categorical(combo_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 54, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 10, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                122944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 179,338\n",
      "Trainable params: 179,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "# Convolutional NN\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,56,1)))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "# Adding a NN Classifier\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# opt = tf.keras.optimizers.RMSprop(learning_rate=0.0002)\n",
    "# model.compile(optimizer = opt,\n",
    "#              loss = 'categorical_crossentropy',\n",
    "#              metrics = ['mae', 'accuracy'])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2618 - accuracy: 0.9167\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 103s 65ms/step - loss: 0.0626 - accuracy: 0.9803\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.0345 - accuracy: 0.9893\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 79s 51ms/step - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 79s 50ms/step - loss: 0.0152 - accuracy: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153cf5430>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          epochs = 5, \n",
    "          batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of predictions\n",
    "y_predict = model.predict(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions to validation labels and return the proportion correctly classified\n",
    "correct = 0\n",
    "\n",
    "for label in range(len(val_labels)):\n",
    "    \n",
    "    if np.argmax(val_labels[label], axis=None, out=None) == np.argmax(y_predict[label], axis=None, out=None):\n",
    "        correct = correct + 1\n",
    "        \n",
    "print(\"Val Accuracy: \" + str(correct/len(val_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss, mae and accuracy\n",
    "eval_model = model.evaluate(val_images, val_labels)\n",
    "eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Trained_Models/Model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Trained_Models/Model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify above by visually inspecting images from the predicted set\n",
    "num_val_images = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 5\n",
      "True: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell repeatedly to see multiple examples\n",
    "check_index = random.randint(0,num_val_images)\n",
    "plt.imshow(val_images[check_index].reshape(28,56), cmap = plt.cm.binary)\n",
    "print(\"Predicted: \" + str(np.argmax(y_predict[check_index])))\n",
    "print(\"True: \" + str(np.argmax(val_labels[check_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ML",
   "language": "python",
   "name": "general_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
