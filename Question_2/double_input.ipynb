{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.datasets import mnist\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a defined number of image paires from a given dataset\n",
    "def get_left_min_image_combinations(number_image_pairs_selected, image_data, image_labels):\n",
    "    \n",
    "    num_images_total = len(image_labels)\n",
    "    \n",
    "    image_list_left = []\n",
    "    image_list_right = []\n",
    "    image_combination_labels = []\n",
    "    \n",
    "    for i in range(number_image_pairs_selected):\n",
    "        \n",
    "        # Draw two image indices from a uniform random distribution\n",
    "        random_index_A = random.randint(0, num_images_total-1)\n",
    "        random_index_B = random.randint(0, num_images_total-1)\n",
    "        \n",
    "        # Randomly choose two images from the dataset\n",
    "        image_A = image_data[random_index_A]\n",
    "        image_B = image_data[random_index_B]\n",
    "        \n",
    "        # Find the minimum between the two labels\n",
    "        label_A = image_labels[random_index_A]\n",
    "        label_B = image_labels[random_index_B]\n",
    "        minimum_label = min(label_A, label_B)\n",
    "        \n",
    "        if label_A < label_B:\n",
    "            image_list_left.append(image_A)\n",
    "            image_list_right.append(image_B)\n",
    "            \n",
    "        elif label_A > label_B: \n",
    "            image_list_left.append(image_B)\n",
    "            image_list_right.append(image_A)\n",
    "           \n",
    "        image_combination_labels.append(minimum_label)\n",
    "        \n",
    "    # Convert image data and labels lists to numpy arrays\n",
    "    image_list_left = np.array(image_list_left)\n",
    "    image_list_right = np.array(image_list_right)\n",
    "    \n",
    "    image_combination_labels = np.array(image_combination_labels)\n",
    "    \n",
    "    return image_list_left, image_list_right , image_combination_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a defined number of image paires from a given dataset\n",
    "def get_right_min_image_combinations(number_image_pairs_selected, image_data, image_labels):\n",
    "    \n",
    "    num_images_total = len(image_labels)\n",
    "    \n",
    "    image_list_left = []\n",
    "    image_list_right = []\n",
    "    image_combination_labels = []\n",
    "    \n",
    "    for i in range(number_image_pairs_selected):\n",
    "        \n",
    "        # Draw two image indices from a uniform random distribution\n",
    "        random_index_A = random.randint(0, num_images_total-1)\n",
    "        random_index_B = random.randint(0, num_images_total-1)\n",
    "        \n",
    "        # Randomly choose two images from the dataset\n",
    "        image_A = image_data[random_index_A]\n",
    "        image_B = image_data[random_index_B]\n",
    "        \n",
    "        # Find the minimum between the two labels\n",
    "        label_A = image_labels[random_index_A]\n",
    "        label_B = image_labels[random_index_B]\n",
    "        minimum_label = min(label_A, label_B)\n",
    "        \n",
    "        if label_A < label_B:\n",
    "            image_list_left.append(image_B)\n",
    "            image_list_right.append(image_A)\n",
    "\n",
    "        elif label_A > label_B: \n",
    "            image_list_left.append(image_A)\n",
    "            image_list_right.append(image_B)\n",
    "   \n",
    "        # Append newly generated image combination and minimum label to list\n",
    "        image_combination_labels.append(minimum_label)\n",
    "        \n",
    "    # Convert image data and labels lists to numpy arrays\n",
    "    image_list_left = np.array(image_list_left)\n",
    "    image_list_right = np.array(image_list_right)\n",
    "    image_combination_labels = np.array(image_combination_labels)\n",
    "    \n",
    "    return image_list_left, image_list_right , image_combination_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into training and test\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set further into training and validation sets\n",
    "sample_size = 40000\n",
    "X_train, X_val, y_train, y_val = train_images[:sample_size], train_images[sample_size:sample_size*2],  train_labels[:sample_size], train_labels[sample_size:sample_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_train_X_1, combo_train_X_2, combo_train_y = get_left_min_image_combinations(40000, X_train, y_train)\n",
    "combo_val_X_1, combo_val_X_2, combo_val_y = get_right_min_image_combinations(20000, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data so they can be fed into the network, one-hot-encode the labels\n",
    "\n",
    "train_images_1 = combo_train_X_1.reshape((combo_train_X_1.shape[0], 28, 28, 1))/ 255\n",
    "train_images_2 = combo_train_X_2.reshape((combo_train_X_2.shape[0], 28, 28, 1))/ 255\n",
    "\n",
    "val_images = combo_val_X_1.reshape((combo_val_X_1.shape[0], 28, 28, 1))/255\n",
    "val_images = combo_val_X_2.reshape((combo_val_X_2.shape[0], 28, 28, 1))/255\n",
    "\n",
    "train_labels = to_categorical(combo_train_y)\n",
    "val_labels = to_categorical(combo_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def build_CNN():\n",
    "\n",
    "    # Convolutional NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,56,1)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    # Adding a NN Classifier\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 54, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 10, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                122944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 179,338\n",
      "Trainable params: 179,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = build_CNN()\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "# Cross-validate\n",
    "# Set up parameters for k-fold cross-validation\n",
    "k = 4\n",
    "num_val_samples = len(train_images)//k\n",
    "\n",
    "num_epochs = 5\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print('processing fold #', i)\n",
    "    val_data = train_images[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "    [train_images[:i * num_val_samples],\n",
    "    train_images[(i+1) * num_val_samples:]],\n",
    "    axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "    [train_labels[:i*num_val_samples],\n",
    "    train_labels[(i+1)*num_val_samples:]],\n",
    "    axis = 0)\n",
    "    \n",
    "    model = build_CNN()\n",
    "    history = model.fit(partial_train_data,\n",
    "              partial_train_targets,\n",
    "              validation_data = (val_data, val_targets),\n",
    "              epochs = num_epochs,\n",
    "              batch_size = 64,\n",
    "              verbose = 0)\n",
    "    \n",
    "    val_mae_history = history.history['val_accuracy']\n",
    "    all_mae_histories.append(val_mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11264999955892563,\n",
       " 0.11620000004768372,\n",
       " 0.11755000054836273,\n",
       " 0.10989999771118164,\n",
       " 0.11084999889135361]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_history = history.history['val_accuracy']\n",
    "val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the average of the per-epoch MAE scores for all folds\n",
    "# Building the history of successive mean K-fold validation scores\n",
    "plt.plot(range(1, len(val_acc_history) + 1), val_acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_mae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 4ms/step - loss: 21.9947 - accuracy: 0.1108 - mae: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[21.99468994140625, 0.11084999889135361, 0.17766912281513214]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print loss, mae and accuracy\n",
    "CNN_eval_model = CNN_model.evaluate(val_images, val_labels)\n",
    "CNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def build_DNN():\n",
    "\n",
    "    # Dense NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512, activation = 'relu', input_shape=(28*56,)))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API version of above\n",
    "inputs = keras.Input(shape=(28*56,))\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               803328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 808,458\n",
      "Trainable params: 808,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN_model = build_DNN()\n",
    "DNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(DNN_model, \"DNN_model.png\", show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 47040000\n  y sizes: 40000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c9579bb948e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m DNN_model.fit(train_images.flatten(), \n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           batch_size = 64)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1628\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1629\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 47040000\n  y sizes: 40000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "DNN_model.fit(train_images.flatten(), \n",
    "          train_labels, \n",
    "          epochs = 5, \n",
    "          batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 7s 10ms/step - loss: 14.6404 - accuracy: 0.1159 - mae: 0.1764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.64042854309082, 0.11590000241994858, 0.17643354833126068]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print loss, mae and accuracy\n",
    "DNN_eval_model = DNN_model.evaluate(val_images, val_labels)\n",
    "DNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 47s 76ms/step - loss: 0.0203 - accuracy: 0.9939 - mae: 0.0018\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.0167 - accuracy: 0.9945 - mae: 0.0015\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.0137 - accuracy: 0.9955 - mae: 0.0012\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.0133 - accuracy: 0.9960 - mae: 0.0011\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.0113 - accuracy: 0.9962 - mae: 9.8396e-04\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0118 - accuracy: 0.9963 - mae: 9.3090e-04\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.0110 - accuracy: 0.9963 - mae: 9.2957e-04\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 50s 81ms/step - loss: 0.0098 - accuracy: 0.9969 - mae: 7.9370e-04\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 50s 79ms/step - loss: 0.0089 - accuracy: 0.9974 - mae: 6.7800e-04\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 52s 83ms/step - loss: 0.0075 - accuracy: 0.9980 - mae: 5.9168e-04\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 22.6241 - accuracy: 0.1116 - mae: 0.1776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22.624094009399414, 0.11155000329017639, 0.17761020362377167]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "DNN_model.fit(train_images, \n",
    "          train_labels, \n",
    "          epochs = 10, \n",
    "          batch_size = 64)\n",
    "\n",
    "# Print loss, mae and accuracy\n",
    "DNN_eval_model = DNN_model.evaluate(val_images, val_labels)\n",
    "DNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(shape=(None,), name=\"title\")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ML",
   "language": "python",
   "name": "general_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
