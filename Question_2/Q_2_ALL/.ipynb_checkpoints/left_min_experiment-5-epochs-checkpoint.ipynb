{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.datasets import mnist\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a defined number of image paires from a given dataset\n",
    "def get_left_min_image_combinations(number_image_pairs_selected, image_data, image_labels):\n",
    "    \n",
    "    num_images_total = len(image_labels)\n",
    "    \n",
    "    image_combination_data = []\n",
    "    image_combination_labels = []\n",
    "    \n",
    "    for i in range(number_image_pairs_selected):\n",
    "        \n",
    "        # Draw two image indices from a uniform random distribution\n",
    "        random_index_A = random.randint(0, num_images_total-1)\n",
    "        random_index_B = random.randint(0, num_images_total-1)\n",
    "        \n",
    "        # Randomly choose two images from the dataset\n",
    "        image_A = image_data[random_index_A]\n",
    "        image_B = image_data[random_index_B]\n",
    "        \n",
    "        # Merge images TODO: this could be a function with several options\n",
    "        combined_images = np.hstack((image_A, image_B))\n",
    "        \n",
    "        # Find the minimum between the two labels\n",
    "        label_A = image_labels[random_index_A]\n",
    "        label_B = image_labels[random_index_B]\n",
    "        minimum_label = min(label_A, label_B)\n",
    "        \n",
    "        if label_A <= label_B:\n",
    "            # Merge images TODO: this could be a function with several options\n",
    "            combined_images = np.hstack((image_A, image_B))\n",
    "        elif label_A > label_B: \n",
    "            # Merge images TODO: this could be a function with several options\n",
    "            combined_images = np.hstack((image_B, image_A))\n",
    "        # Here we want to exclude the images where they are the same on both sides\n",
    "            \n",
    "   \n",
    "        # Append newly generated image combination and minimum label to list\n",
    "        image_combination_data.append(combined_images)\n",
    "        image_combination_labels.append(minimum_label)\n",
    "        \n",
    "    # Convert image data and labels lists to numpy arrays\n",
    "    image_combination_data = np.array(image_combination_data)\n",
    "    image_combination_labels = np.array(image_combination_labels)\n",
    "    \n",
    "    return image_combination_data, image_combination_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a defined number of image paires from a given dataset\n",
    "def get_right_min_image_combinations(number_image_pairs_selected, image_data, image_labels):\n",
    "    \n",
    "    num_images_total = len(image_labels)\n",
    "    \n",
    "    image_combination_data = []\n",
    "    image_combination_labels = []\n",
    "    \n",
    "    for i in range(number_image_pairs_selected):\n",
    "        \n",
    "        # Draw two image indices from a uniform random distribution\n",
    "        random_index_A = random.randint(0, num_images_total-1)\n",
    "        random_index_B = random.randint(0, num_images_total-1)\n",
    "        \n",
    "        # Randomly choose two images from the dataset\n",
    "        image_A = image_data[random_index_A]\n",
    "        image_B = image_data[random_index_B]\n",
    "        \n",
    "        # Merge images TODO: this could be a function with several options\n",
    "        combined_images = np.hstack((image_A, image_B))\n",
    "        \n",
    "        # Find the minimum between the two labels\n",
    "        label_A = image_labels[random_index_A]\n",
    "        label_B = image_labels[random_index_B]\n",
    "        minimum_label = min(label_A, label_B)\n",
    "        \n",
    "        if label_A < label_B:\n",
    "            # Merge images TODO: this could be a function with several options\n",
    "            combined_images = np.hstack((image_B, image_A))\n",
    "        elif label_A > label_B: \n",
    "            # Merge images TODO: this could be a function with several options\n",
    "            combined_images = np.hstack((image_A, image_B))\n",
    "        # Here we want to exclude the images where they are the same on both sides\n",
    "            \n",
    "   \n",
    "        # Append newly generated image combination and minimum label to list\n",
    "        image_combination_data.append(combined_images)\n",
    "        image_combination_labels.append(minimum_label)\n",
    "        \n",
    "    # Convert image data and labels lists to numpy arrays\n",
    "    image_combination_data = np.array(image_combination_data)\n",
    "    image_combination_labels = np.array(image_combination_labels)\n",
    "    \n",
    "    return image_combination_data, image_combination_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into training and test\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set further into training and validation sets\n",
    "sample_size = 40000\n",
    "X_train, X_val, y_train, y_val = train_images[:sample_size], train_images[sample_size:sample_size*2],  train_labels[:sample_size], train_labels[sample_size:sample_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_train_X, combo_train_y = get_left_min_image_combinations(40000, X_train, y_train)\n",
    "combo_val_X, combo_val_y = get_right_min_image_combinations(20000, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data so they can be fed into the network, one-hot-encode the labels\n",
    "\n",
    "train_images = combo_train_X.reshape((combo_train_X.shape[0], 28, 56, 1))\n",
    "train_images = train_images/ 255\n",
    "\n",
    "val_images = combo_val_X.reshape((combo_val_X.shape[0], 28, 56, 1))\n",
    "val_images = val_images/255\n",
    "\n",
    "train_labels = to_categorical(combo_train_y)\n",
    "val_labels = to_categorical(combo_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "\n",
    "num_epochs = 5\n",
    "num_iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def build_CNN():\n",
    "\n",
    "    # Convolutional NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,56,1)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    # Adding a NN Classifier\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 54, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 10, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                122944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 179,338\n",
      "Trainable params: 179,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = build_CNN()\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.1934 - accuracy: 0.9404 - mae: 0.0184\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0390 - accuracy: 0.9879 - mae: 0.0037\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 43s 68ms/step - loss: 0.0239 - accuracy: 0.9924 - mae: 0.0022\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.0159 - accuracy: 0.9948 - mae: 0.0014\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.0099 - accuracy: 0.9966 - mae: 9.7388e-04\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 20.0077 - accuracy: 0.1071 - mae: 0.1784\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 49s 79ms/step - loss: 0.0073 - accuracy: 0.9973 - mae: 6.9121e-04\n",
      "Epoch 2/5\n",
      "266/625 [===========>..................] - ETA: 28s - loss: 0.0058 - accuracy: 0.9984 - mae: 4.5999e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2fc93d7e8cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     CNN_model.fit(train_images, \n\u001b[0m\u001b[1;32m      7\u001b[0m               \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "CNN_accuracy = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    CNN_model.fit(train_images, \n",
    "              train_labels, \n",
    "              epochs = num_epochs, \n",
    "              batch_size = 64)\n",
    "    \n",
    "    # Print loss, mae and accuracy\n",
    "    CNN_eval_model = CNN_model.evaluate(val_images, val_labels)\n",
    "    CNN_accuracy.append(CNN_eval_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.2125 - accuracy: 0.9336 - mae: 0.0202\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.0428 - accuracy: 0.9865 - mae: 0.0041\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0259 - accuracy: 0.9919 - mae: 0.0024\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0161 - accuracy: 0.9949 - mae: 0.0015\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.0108 - accuracy: 0.9962 - mae: 0.0011\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 16.8169 - accuracy: 0.1104 - mae: 0.1777\n"
     ]
    }
   ],
   "source": [
    "CNN_model = build_CNN()\n",
    "CNN_model.fit(train_images, \n",
    "              train_labels, \n",
    "              epochs = num_epochs, \n",
    "              batch_size = 64)\n",
    "\n",
    " # Print loss, mae and accuracy\n",
    "CNN_eval_model = CNN_model.evaluate(val_images, val_labels)\n",
    "CNN_accuracy.append(CNN_eval_model[1])\n",
    "CNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.816865921020508, 0.11044999957084656, 0.17765407264232635]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def build_DNN():\n",
    "\n",
    "    # Convolutional NN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,56,1)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "    # Adding a NN Classifier\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 54, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 12, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 10, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                122944    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 179,338\n",
      "Trainable params: 179,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN_model = build_DNN()\n",
    "DNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 37s 58ms/step - loss: 0.2228 - accuracy: 0.9293 - mae: 0.0212\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.0609 - accuracy: 0.9807 - mae: 0.0058\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0412 - accuracy: 0.9876 - mae: 0.0038\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0299 - accuracy: 0.9902 - mae: 0.0028\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0247 - accuracy: 0.9918 - mae: 0.0023\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 14.0905 - accuracy: 0.1081 - mae: 0.1781\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.0222 - accuracy: 0.9930 - mae: 0.0020\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.0181 - accuracy: 0.9942 - mae: 0.0016\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.0166 - accuracy: 0.9944 - mae: 0.0015\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0144 - accuracy: 0.9956 - mae: 0.0012\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.0131 - accuracy: 0.9956 - mae: 0.0012\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 20.3576 - accuracy: 0.1081 - mae: 0.1783\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.0123 - accuracy: 0.9962 - mae: 9.9952e-04\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.0115 - accuracy: 0.9963 - mae: 9.4559e-04\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.0108 - accuracy: 0.9967 - mae: 8.3488e-04\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0094 - accuracy: 0.9973 - mae: 7.2483e-04\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0101 - accuracy: 0.9973 - mae: 6.6610e-04\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 26.0101 - accuracy: 0.1088 - mae: 0.1782\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0090 - accuracy: 0.9973 - mae: 6.5039e-04\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0080 - accuracy: 0.9976 - mae: 5.8160e-04\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.0083 - accuracy: 0.9975 - mae: 6.0097e-04\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0076 - accuracy: 0.9976 - mae: 5.7799e-04\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0075 - accuracy: 0.9977 - mae: 5.7351e-04\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 32.3305 - accuracy: 0.1072 - mae: 0.1785\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.0066 - accuracy: 0.9980 - mae: 4.9449e-04\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0072 - accuracy: 0.9979 - mae: 4.8803e-04\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0062 - accuracy: 0.9982 - mae: 4.1804e-04\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0081 - accuracy: 0.9981 - mae: 4.4746e-04\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0063 - accuracy: 0.9983 - mae: 4.0336e-04\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 36.2823 - accuracy: 0.1059 - mae: 0.1788\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "DNN_accuracy = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    DNN_model.fit(train_images, \n",
    "              train_labels, \n",
    "              epochs = num_epochs, \n",
    "              batch_size = 64)\n",
    "    \n",
    "    # Print loss, mae and accuracy\n",
    "    DNN_eval_model = DNN_model.evaluate(val_images, val_labels)\n",
    "    DNN_accuracy.append(DNN_eval_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 5ms/step - loss: 36.2823 - accuracy: 0.1059 - mae: 0.1788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36.2823371887207, 0.10585000365972519, 0.17877371609210968]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print loss, mae and accuracy\n",
    "DNN_eval_model = DNN_model.evaluate(val_images, val_labels)\n",
    "DNN_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting that the DNN is slightly better than the CNN at this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLotting\n",
    "plt.title('Comparing validation accuracy between DNN and CNN')\n",
    "plt.bar(['DNN','CNN'], [DNN_eval_model[1],CNN_eval_model[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ML",
   "language": "python",
   "name": "general_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
